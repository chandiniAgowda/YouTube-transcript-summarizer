{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import youtube_transcript_api\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://www.youtube.com/watch?v=Y8Tko2YC5hA\" \n",
    "unique_id = link.split(\"=\")[-1]\n",
    "sub = YouTubeTranscriptApi.get_transcript(unique_id)  \n",
    "subtitle = \" \".join([x['text'] for x in sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "subtitle = subtitle.replace(\"n\",\"\")\n",
    "sentences = sent_tokenize(subtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "organized_sent = {k:v for v,k in enumerate(sentences)}\n",
    "tf_idf = TfidfVectorizer(min_df=2, \n",
    "                                    strip_accents='unicode',\n",
    "                                    max_features=None,\n",
    "                                    lowercase = True,\n",
    "                                    token_pattern=r'w{1,}',\n",
    "                                    ngram_range=(1, 3), \n",
    "                                    use_idf=1,\n",
    "                                    smooth_idf=1,\n",
    "                                    sublinear_tf=1,\n",
    "                                    stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pk123\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:594: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pk123\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['w'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentence_vectors = tf_idf.fit_transform(sentences)\n",
    "sent_scores = np.array(sentence_vectors.sum(axis=1)).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "top_n_sentences = [sentences[index] for index in np.argsort(sent_scores, axis=0)[::-1][:N]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the scored sentences with their indexes as in the subtitle\n",
    "mapped_sentences = [(sentence,organized_sent[sentence]) for sentence in top_n_sentences]\n",
    "# Ordering the top-n sentences in their original order\n",
    "mapped_sentences = sorted(mapped_sentences, key = lambda x: x[1])\n",
    "ordered_sentences = [element[0] for element in mapped_sentences]\n",
    "# joining the ordered sentence\n",
    "summary = \" \".join(ordered_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cpu\n",
      "False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c8fa49bdae44f48ec2e1fa5e3280b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pk123\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pk123\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3b775750bf4eeabc472f4e814f59be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())  # Checks if GPU support is enabled\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "input_tensor = tokenizer.encode( subtitle, return_tensors=\"pt\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     0,   510, 20436,   139,    16,     5,  1437,   232,    18,\n",
       "          6273,  1733,  1023,  2329,   144,  1406,   586,   119,  1023,   784,\n",
       "         11993,  1580,     4,    85,    16,   341,    30,  2257,  5485,     6,\n",
       "         45324,  5003,     6,   414,    10,  7776,  5019,     6,  2850,  5810,\n",
       "          1952,     6,  7678,   995,  2923,     6,  4400,  6014,  1023,   364,\n",
       "         25004,   268,     6,  2329, 15330,  1159,     4,   590, 46439,   139,\n",
       "            47,    64,  6136,  2632,  1272,   939,   540,    86,    19,  4163,\n",
       "          5738,     9,  3260,     4,   370,    64,    67,   304, 46439,   139,\n",
       "             7,  1119,  1437,  3748,     6,  1830,  2329, 14050, 40967,   415,\n",
       "          4544,    25,   157,    25,  2257,  1437,  1296,  1023,    50, 15330,\n",
       "         14157,  1023,     4,    85,    18,    10,   182, 21422,   906, 16708,\n",
       "           352, 28644,   119,  1023,  1437,   784, 11993,  3443,     4,   407,\n",
       "            82,    31, 47850,    90, 41352,   918,   304,    24,    13,    10,\n",
       "          3143,     9,  8558,     4,     2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_tensor = model.generate(input_tensor, max_length=160, min_length=120, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "outputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s><s>Pytho is the  world's fastest growig ad most popular programmig laguage. It is used by software developers, mathematicias, data aalysts, scietists, accoutats, etworkig egieers, ad eve kids. With Pytho you can solve complex problems i less time with fewer lies of code. You can also use Pytho to build  web, mobile ad desktop applicatios as well as software  testig or eve hackig. It's a very begier friedlyprogrammig  laguages. So people from differet disciplies use it for a variety of tasks.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs_tensor[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s><s>Pytho is the  world's fastest growig ad\n",
      "most popular programmig laguage. It is used by\n",
      "software developers, mathematicias, data aalysts,\n",
      "scietists, accoutats, etworkig egieers, ad eve\n",
      "kids. With Pytho you can solve complex problems i\n",
      "less time with fewer lies of code. You can also\n",
      "use Pytho to build  web, mobile ad desktop\n",
      "applicatios as well as software  testig or eve\n",
      "hackig. It's a very begier friedlyprogrammig\n",
      "laguages. So people from differet disciplies use\n",
      "it for a variety of tasks.</s>\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "summary =tokenizer.decode(outputs_tensor[0])\n",
    "wrapped_text = textwrap.fill(summary, width=50)  # 50 characters per line\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94187d7c61bc4f268ebfe12e41088558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pk123\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pk123\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afdf2fd7cbe34bbaaaabf5aeef66fcc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d3ca79b33c42ccb3549e0b7e372b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  14%|#3        | 168M/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d641e2859a4498ac445ff6079b5dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17397eead125422389bff504888e05d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d2ca123472410c9818fa4b0a4ef9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarizer = pipeline('summarization')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
